{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Наивный байесовский классификатор. Работа с текстом в Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZD07JID7AnI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrgCQCmU7HkZ"
      },
      "source": [
        "# Наивный байесовский классификатор"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h0mpUCL7TCe"
      },
      "source": [
        "\n",
        "\n",
        "*   Теорема позволяет найти вероятность того, что именно эта причина привела к наблюдаемому событию\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgoTGkt27yM2"
      },
      "source": [
        "<h1 align=\"center\"> $ P(A|B) = \\frac{ P(B|A)P(A) }{ P(B) } $ </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvXBk6Tn8GPb"
      },
      "source": [
        "* $ P(A|B) $ — вероятность того, что событие $А$ истинно, если событие $B$ истинно\n",
        "* $ P(B|A) $ — вероятность того, что событие $B$ истинно, если событие $A$ истинно\n",
        "* $ P(A) $ — вероятность того, что событие $А$ истинно\n",
        "* $ P(B) $ — вероятность того, что событие $B$ истинно\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPLp3DIH-0N1"
      },
      "source": [
        "#### Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYwsONYlJKPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fefd20-8ad1-4e89-8755-6b84ef42adbf"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "import keras\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
        "                                                      num_words=None,\n",
        "                                                      skip_top=0,\n",
        "                                                      maxlen=None,\n",
        "                                                      seed=113,\n",
        "                                                      start_char=1,\n",
        "                                                      oov_char=2,\n",
        "                                                      index_from=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsOCh209JpRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bf96c6b-a879-45c6-a60a-8aef7b9977a9"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "       ...,\n",
              "       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
              "       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwh0nut-JtQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da75f8ba-42e8-46eb-8d3b-060fb1ee4436"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl9Dvo1gKH0g"
      },
      "source": [
        "def decode_review(sample):\n",
        "    NUM_WORDS = None\n",
        "    INDEX_FROM = 3   # откуда индексирем\n",
        "\n",
        "    train, test = keras.datasets.imdb.load_data(\n",
        "        num_words=NUM_WORDS, index_from=INDEX_FROM\n",
        "    )\n",
        "\n",
        "    word_to_id = keras.datasets.imdb.get_word_index()\n",
        "    word_to_id = {k:(v + INDEX_FROM) for k, v in word_to_id.items()}\n",
        "    word_to_id[\"<PAD>\"] = 0\n",
        "    word_to_id[\"<START>\"] = 1\n",
        "    word_to_id[\"<UNK>\"] = 2\n",
        "    word_to_id[\"<UNUSED>\"] = 3\n",
        "\n",
        "    id_to_word = {value: key for key,value in word_to_id.items()}\n",
        "\n",
        "    return ' '.join(id_to_word[id] for id in sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv9ExHFlKqdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8cceb7-7f4f-4086-cb73-944634bd7cde"
      },
      "source": [
        "print(decode_review(x_train[0]))\n",
        "print(y_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2-M0TlOLxQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46dfb6ae-d1bf-4210-b4f1-6ca8b4976bad"
      },
      "source": [
        "print(decode_review(x_train[20]))\n",
        "print(y_train[20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<START> shown in australia as 'hydrosphere' this incredibly bad movie is so bad that you become hypnotised and have to watch it to the end just to see if it could get any worse and it does the storyline is so predictable it seems written by a high school dramatics class the sets are pathetic but marginally better than the miniatures and the acting is wooden br br the infant 'muppet' seems to have been stolen from the props cupboard of 'total recall' there didn't seem to be a single original idea in the whole movie br br i found this movie to be so bad that i laughed most of the way through br br malcolm mcdowell should hang his head in shame he obviously needed the money\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY1fo4DwMFrW"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Liqxl6O_IeS"
      },
      "source": [
        "#### Создание алгоритма и обучение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pjkuRRWQpy6"
      },
      "source": [
        "collections.defaultdict ничем не отличается от обычного словаря за исключением того, что по умолчанию всегда вызывается функция, возвращающая значение:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-h7FpsOQsRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "759efe1c-d728-4196-a843-0ff1cbfc3558"
      },
      "source": [
        "import collections\n",
        "\n",
        "defdict = collections.defaultdict(list)\n",
        "print(defdict)\n",
        "\n",
        "for i in range(5):\n",
        "    defdict[i].append(i)\n",
        "\n",
        "print(defdict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'list'>, {})\n",
            "defaultdict(<class 'list'>, {0: [0], 1: [1], 2: [2], 3: [3], 4: [4]})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rzocQRBihqd"
      },
      "source": [
        "Пусть у нас есть строка текста $O$. Кроме того, имеются классы $С$, к одному из которых мы должны отнести строку. Нам необходимо найти такой класс с, при котором его вероятность для данной строки была бы максимальна. Найдем этот класс как \n",
        "\n",
        "<h4 align=\"center\"> $ С = argmax_c P(C|O) $ </h4>\n",
        "\n",
        "\n",
        "$argmax_x f(x)$ есть значение $x$, при котором $f(x)$ максимальна\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYpRf-Z-jq3G"
      },
      "source": [
        "Т.к. получить $P(C|O)$ сложно, то легче вычислить косвенные вероятности используя теорему Байеса:\n",
        "\n",
        "<h4 align=\"center\"> $ P(C|O) = \\frac{ P(O|C)P(C) }{ P(O) } $ </h4>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_SBGj_EkBmy"
      },
      "source": [
        "Так как мы ищем максимум от функции, то знаменатель нас не интересует (он в данном случае константа). Кроме того, нужно взглянуть на строку O. Обычно, нет смысла работать со всей строкой. Намного эффективней выделить из нее определенные признаки (features). Таким образом формула примет вид:\n",
        "\n",
        "<h4 align=\"center\"> $ P(C|O) = \\frac{ P(o_1, o_2, ... , o_n | C)P(C) }{ P(o_1, o_2, ... , o_n) } $ </h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFTtcaqjkHTY"
      },
      "source": [
        "Используем «наивное» предположение о том, что переменные O зависят только от класса C, и не зависят друг от друга. Это сильно упрощение, но зачастую это работает. Числитель примет вид:\n",
        "\n",
        "(Числитель эквивалентен совместной вероятности модели, переписываем используя повторные приложения определений условной вероятности)\n",
        "\n",
        "<h4 align=\"center\"> $P(C)P(o_1|C)P(o_2|C)...P(o_n|C)$ </h4>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORQ4Fsi0L4UH"
      },
      "source": [
        "from collections import defaultdict\n",
        "from math import log\n",
        "\n",
        "def train(samples):\n",
        "    classes, freq = defaultdict(lambda:0), defaultdict(lambda:0)\n",
        "    for feats, label in samples:\n",
        "        # находим лэйбл класса и считаем его частоту\n",
        "        classes[label] += 1     \n",
        "        # feats - элемент обучающей выборки, наш обзор     \n",
        "        # [1, 14, 22, 16, 43, ... <- закодированные в словаре наборы слов     \n",
        "        for feat in feats:\n",
        "            # проходимся по всем словам и считаем их частоты (кол-во элементов)\n",
        "            # ключ - (лейбл, слово) (пример (0, 11176): 26 )\n",
        "            freq[label, feat] += 1          # count features frequencies\n",
        "    \n",
        "    # внутри freq содержится записи формата (лейбл(1 или 0 в нашем случае), слово(133)) : кол-во встречаний в тексте\n",
        "    # нормализуем частоты фичей и классов\n",
        "    # распаковываем лейбл и фичу из частот, делим на кол-во элементов этого класса\n",
        "    for label, feat in freq:               \n",
        "        freq[label, feat] /= classes[label] # для получения частоты нормируем на кол-во элементов в классе\n",
        "        \n",
        "    # кол-во элементов для каждого класса делим на кол-во семплов\n",
        "    for c in classes:\n",
        "        classes[c] /= len(samples)\n",
        "\n",
        "    # мы получили classes - соответсвует вероятности P(C)\n",
        "    # и freq - вероятности P(O|C)\n",
        "    return classes, freq                   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erz2oZ4koDVT"
      },
      "source": [
        "В функции train первые пять строк производят подсчет количества классов C, а также частоту появления фич O и С в одном семпле. Вторая часть метода просто нормирует эти частоты. Таким образом на выходе получаются вероятности $P(C)$ и $P(O|C)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOvHzg4QoGpO"
      },
      "source": [
        "def classify(classifier, feats):\n",
        "    classes, prob = classifier\n",
        "    '''\n",
        "    print(classes)\n",
        "    print(prob)\n",
        "    \n",
        "    print(-log(classes[0]) + sum(-log(prob.get((0,feat), 10**(-7))) for feat in feats) )\n",
        "    print(-log(classes[1]) + sum(-log(prob.get((1,feat), 10**(-7))) for feat in feats) )\n",
        "    '''\n",
        "    # max ( P(1|O), P(0|O))\n",
        "    \n",
        "    # argmin(-log(C|O))\n",
        "    return min(\n",
        "        classes.keys(),\n",
        "        key = lambda cl: (\n",
        "            -log(classes[cl]) \n",
        "            + sum(-log(prob.get((cl, feat), 10**(-7))) for feat in feats) \n",
        "        )\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GhSkKFZoVh0"
      },
      "source": [
        "В функции classify происходит поиск наиболее вероятного класса. Тут заменяется произведение вероятностей на сумму логарифмов, взятых с отрицательным знаком, и вычисляется не argmax, а argmin. Переход к логарифмам — распространненный прием чтобы избежать слишком маленьких чисел, которые могли бы получится при произведении вероятностей + логарифм – монотонно возрастающая функция. Логарифм от функции достигнет максимума в той же точке (по оси абсцисс), что и сама функция.\n",
        "Число 10(^-7), которое подставляется в логарифм, это способ избежать нуля в аргументе логарифма (т.к. он будет иначе он будет неопределен)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g09dDI0h7bC9"
      },
      "source": [
        "формат 1 элемента обучающей выборки\n",
        "([1, 2, 3, 123, ... <- слова], 1 <-ответ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVjVpMQ3MG1n"
      },
      "source": [
        "features = [(x_train[i], y_train[i]) for i in range(len(x_train))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLFdGQSxM5At"
      },
      "source": [
        "classifier = train(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j32v1e5mPI6d"
      },
      "source": [
        "----\n",
        "\n",
        "#### Проверка\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs_2RbWvQE1l"
      },
      "source": [
        "Посмотрим на пример негативного обзора"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkLHwDL3PMA1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "ad829c1e-a282-473c-dcd5-ddacd9999341"
      },
      "source": [
        "decode_review(x_test[11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> when i first saw this movie in the theater i was so angry it completely blew in my opinion i didn't see it for a decade then decided what the hell let's see i'm watching all hellraiser movies now to see where it went wrong my guess is it was with sequel 5 that was the first to implement the whole i am in a dream omg i see weird stuff oh noes what is happening oh its a dream oh its not a dream oh wait i see something spooky oh never mind sucky storyline those sequels don't even require the box to be opened or stick to the rules from the first 4 movies that if you saw pinhead you are pretty much screwed and dead the first 3 sticked to this storyline which made it so scary in the first place nothing fantasy nothing weird the box got opened boom they came was the only one that could bargain her way out of it first because of uncle frank then because she had information about the this movie at least attempts to stick to all that even though it was a bad story it was still somewhat hellraiser no i'm pretty sure part 5 was the first part to completely and utterly destroy the hellraiser series now they are remaking 1 and i don't even think i will watch it oh who am i kidding i probably will and probably will be disappointed again\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8h7YDNsPRIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4932245f-4099-4969-c36c-a05861de8614"
      },
      "source": [
        "print('true label', y_test[11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true label 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osw9rVajNARj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853f3f04-4672-465b-c050-76ae7c2b33d7"
      },
      "source": [
        "print('predicted', classify(classifier, (x_test[11])) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12HPoYeqPy92"
      },
      "source": [
        "Проверим как работает на примере позитивного обзора"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZY8tCbhPhx4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "e42a4d40-93bd-4f03-b34e-e54434c242ae"
      },
      "source": [
        "decode_review(x_test[30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> tim robbins did a masterful job directing this film i say this because he avoided convention and cliché he also oversaw superb performances from susan sarandon who won an oscar for her role and sean penn even more amazing robbins doesn't patronize he just tells the story and lets the events play on the viewer's mind this is so effective because it allows the viewer to form his own opinions on the death penalty one of the most controversial subjects of our time without being unfairly manipulated in either direction i can't recommend this film enough 9 10\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJdF0BDIPjJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd30de38-bff3-4135-efc1-d4f2cac4caf3"
      },
      "source": [
        "print('true label', y_test[30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true label 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtzHz_W0P52y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c8a690-a5bf-4625-e83e-10cca0737395"
      },
      "source": [
        "print('predicted', classify(classifier, (x_test[30])) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bToQhej5P9ah"
      },
      "source": [
        "----\n",
        "\n",
        "Надо проверить все это на метриках качества"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAGOfB6xNsDX"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predicted = [classify(classifier, (x_test[i])) for i in range(len(x_test))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLKa76hWOBmt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c77039f-3ade-455a-a5b0-5d8f32c1eac9"
      },
      "source": [
        "predicted[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 1, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvYfNZRgOCsu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47577e23-b2d9-46b1-dc50-17f304d15cff"
      },
      "source": [
        "y_test[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7TYH-3HOKyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12baa273-6c9a-4aab-8b64-2dd77234da01"
      },
      "source": [
        "print(classification_report(y_test, predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.79      0.79     12500\n",
            "           1       0.79      0.79      0.79     12500\n",
            "\n",
            "    accuracy                           0.79     25000\n",
            "   macro avg       0.79      0.79      0.79     25000\n",
            "weighted avg       0.79      0.79      0.79     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSgstOgZHIQA"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf6m3uz5t1hR"
      },
      "source": [
        "# Модификации и реализации Наивного Байеса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U27JL4kT8a6J"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import ComplementNB, MultinomialNB, BernoulliNB, CategoricalNB, GaussianNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ADOWyNnuBvZ"
      },
      "source": [
        "## Multinomial Naive Bayes\n",
        "\n",
        "Полиномиальный наивный байесовский классификатор подходит для классификации с дискретными признаками (например, количество слов для классификации текста). Полиномиальное распределение обычно требует целочисленного количества признаков. Однако, на практике, дробные, такие как tf-idf, также могут работать.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKo0xtONyH09"
      },
      "source": [
        "Оригинальный алгоритм:\n",
        "\n",
        "$$p(f_1,..., f_n|c) = \\prod_{i=1}^n p(f_i|c)$$\n",
        "\n",
        "До этого момента мы ничего не говорили о распределении каждого признака. Другими словами, мы оставили $p(fi | c)$ неопределенным. Термин «Полиномиальный наивный байесовский» просто дает нам знать, что каждый $p (fi | c)$ является многочленным распределением, а не каким-либо другим распределением. Это хорошо работает для данных, которые можно легко преобразовать в счетчики, например, слова в тексте. \n",
        "\n",
        "Наивный байесовский классификатор является общим термином, который относится к условной независимости каждой из функций в модели, в то время как полиномиальный наивный байесовский классификатор является конкретным экземпляром наивного байесовского классификатора, который использует полиномиальное распределение для каждой из функций."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs1y5uuQjan4"
      },
      "source": [
        "MultinomialNB реализует наивный алгоритм Байеса для полиномиально распределенных данных и является одним из двух классических наивных байесовских вариантов, используемых в классификации текста. Распределение параметризовано векторами $θy = (θy_1,…, θy_n)$ для каждого класса $y$, где $n$ - количество признаков (в текстовой классификации - размер словаря), а $θy_i$ - вероятность $P (x_i∣y)$ $i$-го признака появиться в документе, принадлежащем к классу у, в количестве $x_i$ штук.\n",
        "\n",
        "Параметры оцениваются по сглаженной версии максимального правдоподобия, то есть относительного подсчета частоты:\n",
        "\n",
        "$$\\hat{\\theta}_{yi} = \\frac{ N_{y_i} + \\alpha}{N_y + \\alpha n}$$\n",
        "\n",
        "Где $N_{yi} = \\sum_{x \\in T} x_i$ - количество раз, в котором i-й признак появляется в документа класса $y$ в обучающем наборе, а $N_{y} = \\sum_{i=1}^{n} N_{y_i}$ - общее количество всех признаков класса $y$.\n",
        "\n",
        "Сглаживающий параметр $α≥0$ учитывают особенности, связанные с отсутствующими признаками в обучающих выборках, и предотвращают нулевые вероятности в дальнейших вычислениях. Установка $α = 1$ называется сглаживанием Лапласа, а $α <1$ называется сглаживанием Лидстона.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "484-pBPV2Yd8"
      },
      "source": [
        "Тогда вероятность того, что конкретный текст $\\mathbf{x}$, в который слово $i$ входит $x_{i}$ раз, выражается в мультномиальной модели как:\n",
        "\n",
        "$$\n",
        "    P(\\mathbf{x}\\ |\\ Q_y) = \\left(\\sum_{i=1}^{n} x_i\\right)! \\prod_{i=1}^{n} \\frac{1}{x_i!} \\hat \\theta_{iy}^{x_i}\n",
        "$$\n",
        "\n",
        "Испольузя условную независимость слов в тексте (предположение наивного байесовского классификатора), вероятность вхождения слова $x_i$ d документ класса Q определяется как:\n",
        "\n",
        "$$P(x_i|Q_y) = \\hat \\theta_{iy}^{x_i} = \\left(\\frac{N_{iy} + \\alpha}{N_y + \\alpha n}\\right)^{x_i}$$\n",
        "\n",
        "где\n",
        "* $N_y$ - количества слов входящий в документ класса Q\n",
        "* $N_{iy}$ - количество вхождений слова $x_{i}$ в документ класса $Q_y$\n",
        "* $\\alpha$ - параметр сглаживания [0, 1)\n",
        "* $n$ - количество слов из обучающей выборки\n",
        "\n",
        "Второй параметр формулы наивного Байеса: $P(Q_i) = \\frac{N_{y}}{N}$\n",
        "где \n",
        "* $N_{y}$ - количество документов класса y\n",
        "* $N$ - все количество документов\n",
        "\n",
        "Итого получаем:\n",
        "\n",
        "$$arg\\max\\limits_{y}\\left[P(Q_y)\\prod_{i=1}^nP(x_i|Q_y)\\right]$$\n",
        "\n",
        "Переходя к логарифмам:\n",
        "\n",
        "$$arg\\max\\limits_{y}\\left[\\log{P(Q_y)}+\\sum_{i=1}^n\\log{P(x_i|Q_y)}\\right]$$\n",
        "\n",
        "Нетрудно видеть, что после логарифмирования мы получили линейное по $x_i$ выражение, которое легко посчитать"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3DBKmOp7ko5"
      },
      "source": [
        "### Подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUi3Geh05udt"
      },
      "source": [
        "Используем оптимизированный в sklearn на примере русских твиттов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNPMRY3Z6Sn4"
      },
      "source": [
        "Данные взяты из статьи http://www.swsys.ru/index.php?page=article&id=3962&lang=\n",
        "\n",
        "В корпусе 114,911 положительных и 111,923 отрицательных записей. Представляют из себя разметку сентимента русскоязычных твитов. (сорс https://study.mokoron.com/ там есть и на 17 миллионов значений, кто хочет взять нечто подобное для проекта)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f479dvP56A05"
      },
      "source": [
        "neg = pd.read_csv('https://raw.githubusercontent.com/tixonsit/mmdad_materials/master/negative.csv', encoding='utf8', sep=';', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiJ2hEnd7-Sk"
      },
      "source": [
        "pos = pd.read_csv('https://raw.githubusercontent.com/tixonsit/mmdad_materials/master/positive.csv', encoding='utf8', sep=';', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM34g-wH8ShZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23d2c74-1279-48ad-aa37-b073d640bebe"
      },
      "source": [
        "pos[3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         @first_timee хоть я и школота, но поверь, у на...\n",
              "1         Да, все-таки он немного похож на него. Но мой ...\n",
              "2         RT @KatiaCheh: Ну ты идиотка) я испугалась за ...\n",
              "3         RT @digger2912: \"Кто то в углу сидит и погибае...\n",
              "4         @irina_dyshkant Вот что значит страшилка :D\\nН...\n",
              "                                ...                        \n",
              "114906    Спала в родительском доме, на своей кровати......\n",
              "114907    RT @jebesilofyt: Эх... Мы немного решили сокра...\n",
              "114908    Что происходит со мной, когда в эфире #proacti...\n",
              "114909    \"Любимая,я подарю тебе эту звезду...\" Имя како...\n",
              "114910    @Ma_che_rie посмотри #непытайтесьпокинутьомск ...\n",
              "Name: 3, Length: 114911, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARLg_nmi8Wsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725ae627-276c-4a26-a5d2-8d07ffd0fe14"
      },
      "source": [
        "neg[3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         на работе был полный пиддес :| и так каждое за...\n",
              "1         Коллеги сидят рубятся в Urban terror, а я из-з...\n",
              "2         @elina_4post как говорят обещаного три года жд...\n",
              "3         Желаю хорошего полёта и удачной посадки,я буду...\n",
              "4         Обновил за каким-то лешим surf, теперь не рабо...\n",
              "                                ...                        \n",
              "111918    Но не каждый хочет что то исправлять:( http://...\n",
              "111919    скучаю так :-( только @taaannyaaa вправляет мо...\n",
              "111920            Вот и в школу, в говно это идти уже надо(\n",
              "111921    RT @_Them__: @LisaBeroud Тауриэль, не грусти :...\n",
              "111922    Такси везет меня на работу. Раздумываю приплат...\n",
              "Name: 3, Length: 111923, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsV6ouHx6F7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7225688-bb0e-43fb-e5fd-d45fb10482ac"
      },
      "source": [
        "df = pos.append(neg, ignore_index = True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "226834"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv0PQYN86wuC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fd2784f1-0790-4dc7-a610-ea1cc1429309"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>412939535870197760</td>\n",
              "      <td>1387287432</td>\n",
              "      <td>TweeetAmnesia</td>\n",
              "      <td>хны... мои анимешки не переводят:(\\n#пичалька</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7131</td>\n",
              "      <td>101</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>419851338864791552</td>\n",
              "      <td>1388935334</td>\n",
              "      <td>lighblomtoli</td>\n",
              "      <td>бляяя тупое московское время, в 23 часа по мос...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>78</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>411993311508193280</td>\n",
              "      <td>1387061834</td>\n",
              "      <td>amerikandy</td>\n",
              "      <td>На Гарри Поттера подсела(( оторваться не могу....</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>208</td>\n",
              "      <td>46</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>418338425553444864</td>\n",
              "      <td>1388574627</td>\n",
              "      <td>RonyLiss</td>\n",
              "      <td>@vikylya__ тоже ем сладкий подарок и скучаю сижу(</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11487</td>\n",
              "      <td>565</td>\n",
              "      <td>621</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>410470546321985536</td>\n",
              "      <td>1386698779</td>\n",
              "      <td>olgaorange83</td>\n",
              "      <td>Вот откуда любовь к совам....\\nИли к Франции))...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5471</td>\n",
              "      <td>200</td>\n",
              "      <td>149</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0           1              2   ...   9    10  11\n",
              "0  412939535870197760  1387287432  TweeetAmnesia  ...  101   49   0\n",
              "1  419851338864791552  1388935334   lighblomtoli  ...   78  165   0\n",
              "2  411993311508193280  1387061834     amerikandy  ...   46   48   0\n",
              "3  418338425553444864  1388574627       RonyLiss  ...  565  621   1\n",
              "4  410470546321985536  1386698779   olgaorange83  ...  200  149   1\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDcBjpy8-f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6469c42e-9fa5-4946-9371-dfbeb6df0e75"
      },
      "source": [
        "# сделаем привычные 1, 0\n",
        "df[4] = df[4].map({-1:0, 1:1})\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>412939535870197760</td>\n",
              "      <td>1387287432</td>\n",
              "      <td>TweeetAmnesia</td>\n",
              "      <td>хны... мои анимешки не переводят:(\\n#пичалька</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7131</td>\n",
              "      <td>101</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>419851338864791552</td>\n",
              "      <td>1388935334</td>\n",
              "      <td>lighblomtoli</td>\n",
              "      <td>бляяя тупое московское время, в 23 часа по мос...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>78</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>411993311508193280</td>\n",
              "      <td>1387061834</td>\n",
              "      <td>amerikandy</td>\n",
              "      <td>На Гарри Поттера подсела(( оторваться не могу....</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>208</td>\n",
              "      <td>46</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>418338425553444864</td>\n",
              "      <td>1388574627</td>\n",
              "      <td>RonyLiss</td>\n",
              "      <td>@vikylya__ тоже ем сладкий подарок и скучаю сижу(</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11487</td>\n",
              "      <td>565</td>\n",
              "      <td>621</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>410470546321985536</td>\n",
              "      <td>1386698779</td>\n",
              "      <td>olgaorange83</td>\n",
              "      <td>Вот откуда любовь к совам....\\nИли к Франции))...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5471</td>\n",
              "      <td>200</td>\n",
              "      <td>149</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0           1              2   ...   9    10  11\n",
              "0  412939535870197760  1387287432  TweeetAmnesia  ...  101   49   0\n",
              "1  419851338864791552  1388935334   lighblomtoli  ...   78  165   0\n",
              "2  411993311508193280  1387061834     amerikandy  ...   46   48   0\n",
              "3  418338425553444864  1388574627       RonyLiss  ...  565  621   1\n",
              "4  410470546321985536  1386698779   olgaorange83  ...  200  149   1\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFJXXSbx6dkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b783be0-1586-468f-e6e3-400fedf846cc"
      },
      "source": [
        "df[4].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    114911\n",
              "0    111923\n",
              "Name: 4, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shHzKBii_WOy"
      },
      "source": [
        "## Кодирования текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okgqtPA07beS"
      },
      "source": [
        "### Векторное представление текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAic0i7x7q1d"
      },
      "source": [
        "Представим текст как вектор индикаторов вхождений слов из некоторого словаря в текст. Простейшая модель BOF (Bag of words или мешок слов)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdd-2tTr7AYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b8c945-5ff7-4991-bf40-64fc5501610a"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "vectorizer.fit(df[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IaTKuDi7Af4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e555f5d1-01fd-4c0d-a2f1-3b6cbb57c1a4"
      },
      "source": [
        "len(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "294600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYQIs-1m7-xS"
      },
      "source": [
        "### Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfwThI317-PO"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCrFC1sf7Aic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf3df01-ad36-498b-f504-39f2d2227d2a"
      },
      "source": [
        "df_train.shape, df_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((181467, 12), (45367, 12))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyi7hNjS8gm_"
      },
      "source": [
        "X_train = vectorizer.transform(df_train[3])\n",
        "X_test = vectorizer.transform(df_test[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh24iglJ7Alc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282c4cb2-a669-4d26-838a-b03a5e885d20"
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((181467, 294600), (45367, 294600))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmBF9qSF8tDS"
      },
      "source": [
        "### Алгоритм"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6lImfg78sYi"
      },
      "source": [
        "clf = MultinomialNB().fit(X_train, df_train[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZvo3F547Any",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e51593d-6a9a-4900-9776-d4e0d81252c3"
      },
      "source": [
        "# выбрать n лучших (по вероятностям) слов для каждого класса\n",
        "def show_top(classifier, vectorizer, n, categories=('pos', 'neg')):\n",
        "    # получаем слова\n",
        "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
        "    # проходимлся по словам\n",
        "    for i, category in enumerate(categories):\n",
        "        # выбираем лучшие по вероятностям\n",
        "        top = np.argsort(classifier.feature_log_prob_[i])[-n:]\n",
        "        print(\"%s: %s\" % (category, \" \".join(feature_names[top])))\n",
        "\n",
        "# отобразим 20 топовых слов\n",
        "show_top(clf, vectorizer, 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos: же ты за по уже ну это все но то так мне co http как меня rt на что не\n",
            "neg: уже да за по ну но все мне так ты меня то это как что на rt co http не\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY8ZwEJM-Uf3"
      },
      "source": [
        "### Оценка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAyFS1FM-Tu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e3db23-0fd5-4bb0-9f0f-f0cf44e36766"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predicts = clf.predict(X_train)\n",
        "# train\n",
        "print(classification_report(df_train[4], predicts, target_names=['Негативные', 'Позитивные']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Негативные       0.87      0.93      0.90     89415\n",
            "  Позитивные       0.92      0.86      0.89     92052\n",
            "\n",
            "    accuracy                           0.89    181467\n",
            "   macro avg       0.90      0.89      0.89    181467\n",
            "weighted avg       0.90      0.89      0.89    181467\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDuWeKri_DPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1903c39a-8a9a-4973-9b60-ed28e3adf47e"
      },
      "source": [
        "predicts = clf.predict(X_test)\n",
        "print(classification_report(df_test[4], predicts, target_names=['Негативные', 'Позитивные']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Негативные       0.73      0.80      0.77     22508\n",
            "  Позитивные       0.78      0.71      0.75     22859\n",
            "\n",
            "    accuracy                           0.76     45367\n",
            "   macro avg       0.76      0.76      0.76     45367\n",
            "weighted avg       0.76      0.76      0.76     45367\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG2T9bUAbvd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31687d8-fe1e-490d-fc86-f58b1d402dc9"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(df_test[4], predicts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[18025,  4483],\n",
              "       [ 6540, 16319]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWRyi0bX_ai4"
      },
      "source": [
        "### Count vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQmMlfwS_fba"
      },
      "source": [
        "При построении вектора признаков будем учитывать не просто факт вхождения слова в текст, а подсчитывать количество вхождений:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_cvKBRF_dUc"
      },
      "source": [
        "count_vect = CountVectorizer(binary=False).fit(df[3])\n",
        "\n",
        "X_train_counts = count_vect.transform(df_train[3])\n",
        "X_test_counts = count_vect.transform(df_test[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDDGDLwM_pOY"
      },
      "source": [
        "Для примера выведем слова вместе и количеством вхождений для обучающего датасета:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvSAJa78_vmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ebcaeb-d236-4d5a-aa9c-b30c54e9cd8d"
      },
      "source": [
        "# представление 42го отзыва\n",
        "dict(zip(count_vect.inverse_transform(X_train_counts[182])[0], X_train_counts[0].data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'xa33a_b_kocmoce': 1,\n",
              " 'интересно': 1,\n",
              " 'милки': 1,\n",
              " 'мне': 1,\n",
              " 'пиши': 1,\n",
              " 'попробовать': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN5HKbk-DQ89"
      },
      "source": [
        "### Алгоритм"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzHN8bQRAImd"
      },
      "source": [
        "# обучим\n",
        "clf = MultinomialNB().fit(X_train_counts, df_train[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTWYsdJkAOGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24541afb-0481-40af-ecb4-cdc95e962085"
      },
      "source": [
        "# train\n",
        "predicts = clf.predict(X_train_counts)\n",
        "print(classification_report(df_train[4], predicts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.89     89415\n",
            "           1       0.92      0.86      0.89     92052\n",
            "\n",
            "    accuracy                           0.89    181467\n",
            "   macro avg       0.89      0.89      0.89    181467\n",
            "weighted avg       0.89      0.89      0.89    181467\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25gt01i_AOZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a92707-755c-4110-bef9-43e045e776e1"
      },
      "source": [
        "# test\n",
        "# конвертируем тестовые слова\n",
        "predicts = clf.predict(count_vect.transform(df_test[3]))\n",
        "print(classification_report(df_test[4], predicts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.80      0.77     22508\n",
            "           1       0.78      0.71      0.75     22859\n",
            "\n",
            "    accuracy                           0.76     45367\n",
            "   macro avg       0.76      0.76      0.76     45367\n",
            "weighted avg       0.76      0.76      0.76     45367\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbGJWoJnAl0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5afed761-959e-430f-838a-0ccf238f61c2"
      },
      "source": [
        "show_top(clf, vectorizer, 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos: хочу за уже ты по ну это все но так co http мне то как меня rt на что не\n",
            "neg: вот да за по ну но мне все так меня ты то это как что на rt co http не\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7BzbQCBDYh_"
      },
      "source": [
        "Пробуем убрать часто встречающиеся слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOtlgaKEA_jI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f7954a-8b8e-4a7f-9a2f-267226f07b8f"
      },
      "source": [
        "'''\n",
        "в англ:\n",
        "\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "\n",
        "count_vect = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, binary=False).fit(df[3])\n",
        "'''\n",
        "!pip install --user -U nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: nltk in /root/.local/lib/python3.6/site-packages (3.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV_Lu4raCV_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4ab783-ad66-4419-aa21-065dfa49426b"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oNPQ1WDexZU"
      },
      "source": [
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkSqU1mhCEo6"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopWords = stopwords.words('russian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uA8Hbx9BzCv"
      },
      "source": [
        "count_vect = CountVectorizer(stop_words=stopWords + list(ENGLISH_STOP_WORDS), binary=False).fit(df[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnfp_nMqCwPY"
      },
      "source": [
        "X_train_counts = count_vect.transform(df_train[3])\n",
        "X_test_counts = count_vect.transform(df_test[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wJIQNOvC5XS"
      },
      "source": [
        "clf = MultinomialNB().fit(X_train_counts, df_train[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkvO5YjLC932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f6fc73-7f44-47c9-c754-e054d2d58b08"
      },
      "source": [
        "predicts = clf.predict(X_test_counts)\n",
        "print(classification_report(df_test[4], predicts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.78      0.76     22508\n",
            "           1       0.77      0.72      0.75     22859\n",
            "\n",
            "    accuracy                           0.75     45367\n",
            "   macro avg       0.75      0.75      0.75     45367\n",
            "weighted avg       0.75      0.75      0.75     45367\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SVlY5jrDDu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f13de5d-b95e-432e-907a-060e5a88a9fe"
      },
      "source": [
        "show_top(clf, count_vect, 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos: год дома спать знаю тебе буду всё вообще почему просто могу завтра блин день очень сегодня хочу это http rt\n",
            "neg: время ещё ахах знаю хочу вообще буду завтра всем всё люблю день очень спасибо тебе просто сегодня это rt http\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjoAipYTDJI7"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDih79PbDdm6"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# vectorizer = TfidfVectorizer(stop_words=stopWords + list(ENGLISH_STOP_WORDS))\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer = vectorizer.fit(df[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWuLArRSDdpx"
      },
      "source": [
        "X_train_vectors = vectorizer.transform(df_train[3])\n",
        "X_test_vectors = vectorizer.transform(df_test[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0Yk36HoDdsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc37496-b338-4173-e51a-35f39234f77b"
      },
      "source": [
        "num = 65\n",
        "X_train_vectors[num].data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25867307, 0.14001284, 0.33939423, 0.27961954, 0.3233279 ,\n",
              "       0.13538313, 0.06702656, 0.21047294, 0.17524024, 0.32592152,\n",
              "       0.3767364 , 0.29915581, 0.27486131, 0.3233279 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjHaTY29Dppp"
      },
      "source": [
        "Выведем слова из первого документа в порядке увеличения из меры TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdX4CNBTDqiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ca718d-8bbc-47a2-c6e5-2284ca20909c"
      },
      "source": [
        "vectorizer.inverse_transform(X_train_vectors[num])[0][np.argsort(X_train_vectors[num].data)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['не', 'он', 'тебе', 'конечно', 'наш', 'тяжелый', 'nalbakrinova',\n",
              "       'подошел', 'rufoteev', 'питерский', 'lualebedin', 'климат',\n",
              "       'сырой', 'ветреный'], dtype='<U136')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ1w2Ha-DzOH"
      },
      "source": [
        "### Алгоритм"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TGTHKy7D0yH"
      },
      "source": [
        "clf = MultinomialNB().fit(X_train_vectors, df_train[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91v51qIzDyuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf825351-326b-4e07-ac0f-38395a86d67a"
      },
      "source": [
        "show_top(clf, vectorizer, 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos: же ты нет ну уже хочу это http co все но то мне так как rt меня на что не\n",
            "neg: вот за по но да ну мне так все меня то ты как это что на rt не http co\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvUvUVzBD5zy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a58fdeed-710b-4c0f-b896-490eb9dec863"
      },
      "source": [
        "predicts = clf.predict(vectorizer.transform(df_test[3]))\n",
        "print(classification_report(df_test[4], predicts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.80      0.76     22508\n",
            "           1       0.78      0.72      0.75     22859\n",
            "\n",
            "    accuracy                           0.76     45367\n",
            "   macro avg       0.76      0.76      0.76     45367\n",
            "weighted avg       0.76      0.76      0.76     45367\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHQSHhfRECA9"
      },
      "source": [
        "### TF-IDF + пары слов\n",
        "\n",
        "Попробуем подсчитывать не только одиночные слова, но и пары слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euRBH3YQEC9C"
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2)).fit(df[3])\n",
        "\n",
        "X_train_vectors = vectorizer.transform(df_train[3])\n",
        "X_test_vectors = vectorizer.transform(df_test[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv2pVO3UN3nq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbfc6649-d2cb-42eb-88c0-29f656bc6317"
      },
      "source": [
        "X_train_vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(181467, 1572672)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyqXGOnxFITc"
      },
      "source": [
        "Выведем слова из первого документа в порядке увеличения из меры TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bB3hAVFFHmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d379aa6-f3c6-41f7-c696-61a43a22ec3f"
      },
      "source": [
        "vectorizer.inverse_transform(X_train_vectors[num])[0][np.argsort(X_train_vectors[num].data)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['не', 'он', 'тебе', 'конечно', 'наш', 'тяжелый', 'nalbakrinova',\n",
              "       'подошел', 'rufoteev', 'он конечно', 'lualebedin', 'питерский',\n",
              "       'климат', 'сырой', 'не подошел', 'тебе наш', 'lualebedin rufoteev',\n",
              "       'rufoteev nalbakrinova', 'тяжелый он', 'питерский климат',\n",
              "       'наш питерский', 'подошел тебе', 'конечно сырой', 'климат тяжелый',\n",
              "       'ветреный', 'сырой ветреный', 'nalbakrinova не'], dtype='<U136')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTcpfWQGIJ95"
      },
      "source": [
        "### Алгоритм"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1g9NQTRFQu2"
      },
      "source": [
        "clf = MultinomialNB(alpha = 0.5).fit(X_train_vectors, df_train[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQt9vDT6FWDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf4048b-690c-4d6a-808f-a6a2c3818154"
      },
      "source": [
        "predicts = clf.predict(vectorizer.transform(df_test[3]))\n",
        "print(classification_report(df_test[4], predicts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.82      0.78     22508\n",
            "           1       0.80      0.72      0.76     22859\n",
            "\n",
            "    accuracy                           0.77     45367\n",
            "   macro avg       0.77      0.77      0.77     45367\n",
            "weighted avg       0.77      0.77      0.77     45367\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49_MZNkivJNG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}